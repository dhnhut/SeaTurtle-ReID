{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7fe4985",
   "metadata": {},
   "source": [
    "# Experiment 01\n",
    "\n",
    "- Loss function: ArcFace\n",
    "- SwimB\n",
    "- closed set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0367ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66673ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "\n",
    "from src.dataset import SeaTurtleDataset\n",
    "from src.arcface import ArcFace\n",
    "from src.utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4796a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "BATCH_SIZE = 10\n",
    "IMG_SIZE = 224\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c78a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir='../data'\n",
    "\n",
    "train_csv_path = os.path.join(dataset_dir, \"metadata_splits_filtered_closed_train.csv\")\n",
    "eval_csv_path = os.path.join(dataset_dir, \"metadata_splits_filtered_closed_eval.csv\")\n",
    "test_csv_path = os.path.join(dataset_dir, \"metadata_splits_filtered_closed_test.csv\")\n",
    "\n",
    "train_dataset = SeaTurtleDataset(annotations_file=train_csv_path, img_dir=dataset_dir, transform=train_transform)\n",
    "eval_dataset = SeaTurtleDataset(annotations_file=eval_csv_path, img_dir=dataset_dir, transform=test_transform)\n",
    "test_dataset = SeaTurtleDataset(annotations_file=test_csv_path, img_dir=dataset_dir, transform=test_transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab7b0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Original head: Linear(in_features=1024, out_features=512, bias=True)\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 512\n",
    "NUM_CLASSES = train_dataset.img_annotations['identity'].nunique()\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "device = get_device()\n",
    "model_save_path = '../models/filtered_closed_arcface_swin_b.pth'\n",
    "\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# --- Swin B Backbone Model ---\n",
    "model = models.swin_b(weights=models.Swin_B_Weights.IMAGENET1K_V1)\n",
    "# Replace the final classification head with a layer that produces the embeddings\n",
    "model.head = nn.Linear(model.head.in_features, EMBEDDING_SIZE)\n",
    "print(\"Original head:\", model.head)\n",
    "print(model.head.in_features)\n",
    "model.to(device)\n",
    "\n",
    "# --- ArcFace Head & Loss Func ---\n",
    "metric = ArcFace(num_classes=NUM_CLASSES, embedding_size=EMBEDDING_SIZE, scale=30.0, margin=0.50).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Optimizer ---\n",
    "optimizer = optim.AdamW(\n",
    "    list(model.parameters()) + list(metric.parameters()),\n",
    "    lr=LEARNING_RATE\n",
    ")\n",
    "\n",
    "# --- Scheduler ---\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21071799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2535d851bbfc4492b960f63af0c9556c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 0.0000, Test Accuracy: 0.00%\n",
      "Finished Training. Best Test Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "best_acc = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for i, (images, labels, _identities) in enumerate(progress_bar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        features = model(images)\n",
    "        output = metric(features, labels)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': running_loss / (i + 1)})\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _identities in eval_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            embeddings = model(images)\n",
    "            # For evaluation, we can just use the embeddings and a KNN classifier, but for simplicity,\n",
    "            # let's use the ArcFace output directly with a simple classification check.\n",
    "            # A more accurate reproduction would involve creating a gallery of embeddings from the training set.\n",
    "            outputs = metric(embeddings, labels)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {running_loss/len(train_loader):.4f}, Test Accuracy: {epoch_acc:.2f}%\")\n",
    "    \n",
    "    if epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(\"Saved best model.\")\n",
    "\n",
    "\n",
    "print(f\"Finished Training. Best Test Accuracy: {best_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
